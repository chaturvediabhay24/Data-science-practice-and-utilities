{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "prOYzALXt9HW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "132bbacc-722c-46b0-f51c-6140a458a7c6"
      },
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self, m=1, c=1):\n",
        "    self.m=m\n",
        "    self.c=c\n",
        "  def infer(self, x):\n",
        "    return (self.m*x)+self.c\n",
        "\n",
        "  def mean_squared_error(self, X, Y):\n",
        "    n=len(X)\n",
        "    squared_error=0\n",
        "    for i in range(n):\n",
        "      squared_error+=((Y[i]-self.infer(X[i]))**2)\n",
        "    return squared_error/n\n",
        "\n",
        "  def gradient_descent(self, X, Y, alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        diff=Y[i]-self.infer(X[i])\n",
        "        dm+=(X[i]*diff)\n",
        "        dc+=(diff)\n",
        "\n",
        "      D_m=(-2/n)*(dm)\n",
        "      D_c=(-2/n)*(dc)\n",
        "\n",
        "      self.m-=alpha*D_m\n",
        "      self.c-=alpha*D_c\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"MSE : {}\".format(self.mean_squared_error(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def momentum_optimizer(self, X, Y, beta=0.9, alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        diff=Y[i]-self.infer(X[i])\n",
        "        dm+=(X[i]*diff)\n",
        "        dc+=(diff)\n",
        "\n",
        "      dm=(-2/n)*(dm)\n",
        "      dc=(-2/n)*(dc)\n",
        "\n",
        "      D_m=beta*D_m +alpha*dm \n",
        "      D_c=beta*D_c +alpha*dc \n",
        "\n",
        "\n",
        "      self.m-=D_m\n",
        "      self.c-=D_c\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"MSE : {}\".format(self.mean_squared_error(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "\n",
        "  def nesterov_acc_gradient(self, X, Y, beta=0.9, alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        diff=Y[i]-self.infer(X[i]+beta*D_m)\n",
        "        dm+=((X[i]+beta*D_m)*diff)\n",
        "        dc+=(diff)\n",
        "\n",
        "      D_m=beta*D_m+alpha*(-2/n)*(dm)\n",
        "      D_c=beta*D_c+alpha*(-2/n)*(dc)\n",
        "\n",
        "      self.m-=D_m\n",
        "      self.c-=D_c\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"MSE : {}\".format(self.mean_squared_error(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "\n",
        "  def adagrad(self, X, Y, epsilon=10**(-10), alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    sm=0\n",
        "    sc=0\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        diff=Y[i]-self.infer(X[i])\n",
        "        dm+=(X[i]*diff)\n",
        "        dc+=(diff)\n",
        "\n",
        "      dm=(-2/n)*(dm)\n",
        "      dc=(-2/n)*(dc)\n",
        "\n",
        "      sm+=dm*dm\n",
        "      sc+=dc*dc\n",
        " \n",
        "\n",
        "      self.m-=alpha*(dm)/math.sqrt(sm+epsilon)\n",
        "      self.c-=alpha*(dc)/math.sqrt(sc+epsilon)\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"log loss : {}\".format(self.mean_squared_error(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def rms_prop(self, X, Y, beta=0.9, epsilon=10**(-10), alpha=1e-4, epochs=10):\n",
        "      n=len(X)\n",
        "      D_m=0\n",
        "      D_c=0\n",
        "      sm=0\n",
        "      sc=0\n",
        "      for _ in range(epochs):\n",
        "        dm=0\n",
        "        dc=0\n",
        "        for i in range(n):\n",
        "          diff=Y[i]-self.infer(X[i])\n",
        "          dm+=(X[i]*diff)\n",
        "          dc+=(diff)\n",
        "\n",
        "        dm=(-2/n)*(dm)\n",
        "        dc=(-2/n)*(dc)\n",
        "\n",
        "        sm=beta*sm+(1-beta)*dm*dm\n",
        "        sc=beta*sc+(1-beta)*dc*dc\n",
        "\n",
        "        self.m+=alpha*dm/math.sqrt(sm+epsilon)\n",
        "        self.c+=alpha*dc/math.sqrt(sc+epsilon)\n",
        "\n",
        "      print(\"log loss : {}\".format(self.mean_squared_error(X,Y)))\n",
        "      print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X=[1,2,3,4,5,6,7,8,9,10]\n",
        "Y=[1,2,3,4,5,6,7,8,9,10]\n",
        "node=LinearRegression()\n",
        "node.infer(2)\n",
        "node.gradient_descent(X, Y, epochs=100)\n",
        "print(node.infer(2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE : 0.18915506315972802\n",
            "m : 0.8650915993512205, c = 0.9394874223437868 \n",
            "2.6696706210462278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUyM8hk38wbo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "876e932a-b8a8-4590-ece4-81689a568640"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X=[1,2,3,4,5,6,7,8,9,10]\n",
        "Y=[1,2,3,4,5,6,7,8,9,10]\n",
        "node=LinearRegression()\n",
        "node.infer(2)\n",
        "node.momentum_optimizer(X, Y, epochs=100)\n",
        "print(node.infer(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE : 0.09320736494723722\n",
            "m : 0.9045580716605591, c = 0.6593061283579712 \n",
            "2.4684222716790893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vuv5GBQiXHS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "66b80a70-263e-4a51-ae26-e02afdc23449"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X=[1,2,3,4,5,6,7,8,9,10]\n",
        "Y=[1,2,3,4,5,6,7,8,9,10]\n",
        "node=LinearRegression()\n",
        "node.infer(2)\n",
        "node.nesterov_acc_gradient(X, Y, epochs=100)\n",
        "print(node.infer(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE : 0.09320823452967812\n",
            "m : 0.9041545953696912, c = 0.6591380447321176 \n",
            "2.4674472354715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "412_i2nUv23F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a179586f-c751-4f22-c626-a377d52ed47c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X=[1,2,3,4,5,6,7,8,9,10]\n",
        "Y=[1,2,3,4,5,6,7,8,9,10]\n",
        "node=LinearRegression()\n",
        "node.infer(2)\n",
        "node.adagrad(X, Y, epochs=100)\n",
        "print(node.infer(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.7815396525185713\n",
            "m : 0.9819431507899943, c = 0.9818373114671616 \n",
            "2.94572361304715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiPvtJQ50dJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "12707939-0db9-49ac-f5e4-cde1d650face"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "X=[1,2,3,4,5,6,7,8,9,10]\n",
        "Y=[1,2,3,4,5,6,7,8,9,10]\n",
        "node=LinearRegression()\n",
        "node.infer(2)\n",
        "node.rms_prop(X, Y, epochs=100)\n",
        "print(node.infer(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 1.1487753729805197\n",
            "m : 1.0109780022553356, c = 1.010966455490579 \n",
            "3.0329224600012505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF8tkSGlwz1S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "64ae33e3-dc36-4354-8ab3-29d491216fbb"
      },
      "source": [
        "import math\n",
        "class LogisticRegression(LinearRegression):\n",
        "  def infer(self, x):\n",
        "    X= (self.m*x)+self.c\n",
        "    return (1/(1+math.exp(-X)))\n",
        "\n",
        "  def __log_loss(self, X, Y):\n",
        "    n=len(X)\n",
        "    loss=0\n",
        "    for i in range(n):\n",
        "      y_pred=self.infer(X[i])\n",
        "      loss+=Y[i]*math.log(y_pred)+(1-Y[i])*math.log(1-y_pred)\n",
        "    return -loss/n\n",
        "\n",
        "  def gradient_descent(self, X, Y, alpha=1e-5, epochs=10):\n",
        "    n=len(X)\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        y_pred=self.infer(X[i])\n",
        "        diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "        dm+=(X[i]*diff)\n",
        "        dc+=diff\n",
        "               \n",
        "\n",
        "      self.m+=alpha*dm\n",
        "      self.c+=alpha*dc\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def momentum_optimizer(self, X, Y, beta=0.9, alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    for _ in range(epochs):\n",
        "      d_m=0\n",
        "      d_c=0\n",
        "      for i in range(n):\n",
        "        y_pred=self.infer(X[i])\n",
        "        diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "        d_m+=(X[i]*diff)\n",
        "        d_c+=diff\n",
        "\n",
        "      D_m=beta*D_m +alpha*d_m \n",
        "      D_c=beta*D_c +alpha*d_c   \n",
        "\n",
        "      self.m+=D_m\n",
        "      self.c+=D_c\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def nesterov_acc_gradient(self, X, Y, beta=0.9, alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    for _ in range(epochs):\n",
        "      d_m=0\n",
        "      d_c=0\n",
        "      for i in range(n):\n",
        "        y_pred=self.infer(X[i]+beta*D_m)\n",
        "        diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "        d_m+=((X[i]+beta*D_m)*diff)\n",
        "        d_c+=diff\n",
        "\n",
        "      D_m=beta*D_m +alpha*d_m \n",
        "      D_c=beta*D_c +alpha*d_c   \n",
        "\n",
        "      self.m+=D_m\n",
        "      self.c+=D_c\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def adagrad(self, X, Y, epsilon=10**(-10), alpha=1e-3, epochs=10):\n",
        "    n=len(X)\n",
        "    D_m=0\n",
        "    D_c=0\n",
        "    sm=0\n",
        "    sc=0\n",
        "    for _ in range(epochs):\n",
        "      dm=0\n",
        "      dc=0\n",
        "      for i in range(n):\n",
        "        y_pred=self.infer(X[i])\n",
        "        diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "        dm+=((X[i])*diff)\n",
        "        dc+=diff\n",
        "\n",
        "      sm+=dm*dm\n",
        "      sc+=dc*dc\n",
        " \n",
        "\n",
        "      self.m+=alpha*(dm)/math.sqrt(sm+epsilon)\n",
        "      self.c+=alpha*(dc)/math.sqrt(sc+epsilon)\n",
        "\n",
        "      # print(\"Epoch : {}\".format(_+1))\n",
        "\n",
        "    print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "    print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def rms_prop(self, X, Y, beta=0.9, epsilon=10**(-10), alpha=1e-3, epochs=10):\n",
        "      n=len(X)\n",
        "      D_m=0\n",
        "      D_c=0\n",
        "      sm=0\n",
        "      sc=0\n",
        "      for _ in range(epochs):\n",
        "        dm=0\n",
        "        dc=0\n",
        "        for i in range(n):\n",
        "          y_pred=self.infer(X[i])\n",
        "          diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "          dm+=((X[i])*diff)\n",
        "          dc+=diff\n",
        "\n",
        "        sm=beta*sm+(1-beta)*dm*dm\n",
        "        sc=beta*sc+(1-beta)*dc*dc\n",
        "\n",
        "        self.m+=alpha*dm/math.sqrt(sm+epsilon)\n",
        "        self.c+=alpha*dc/math.sqrt(sc+epsilon)\n",
        "\n",
        "      print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "      print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "  def adam(self, X, Y, beta1=0.9, beta2=0.999, epsilon=10**(-8), alpha=1e-1, epochs=10):\n",
        "      n=len(X)\n",
        "      Mm=0\n",
        "      Mc=0\n",
        "      Sm=0\n",
        "      Sc=0\n",
        "      for _ in range(epochs):\n",
        "        dm=0\n",
        "        dc=0\n",
        "        for i in range(n):\n",
        "          y_pred=self.infer(X[i])\n",
        "          diff=(Y[i]-y_pred)*y_pred*(1-y_pred)\n",
        "          dm+=((X[i])*diff)\n",
        "          dc+=diff\n",
        "\n",
        "        Mm=beta1*Mm+(1-beta1)*dm\n",
        "        Mc=beta1*Mc+(1-beta1)*dc\n",
        "\n",
        "        Sm=beta2*Sm+(1-beta2)*dm*dm\n",
        "        Sc=beta2*Sc+(1-beta2)*dc*dc\n",
        "\n",
        "        Mm=Mm/(1-beta1**(_+1))\n",
        "        Mc=Mc/(1-beta1**(_+1))\n",
        "\n",
        "        Sm=Sm/(1-beta2**(_+1))\n",
        "        Sc=Sc/(1-beta2**(_+1))\n",
        "\n",
        "        self.m+=alpha*Mm/math.sqrt(Sm+epsilon)\n",
        "        self.c+=alpha*Mc/math.sqrt(Sc+epsilon)\n",
        "\n",
        "      print(\"log loss : {}\".format(self.__log_loss(X,Y)))\n",
        "      print(\"m : {}, c = {} \".format(self.m, self.c))\n",
        "\n",
        "\n",
        "\n",
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.gradient_descent(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.051740240323722986\n",
            "m : 1.0001280735753315, c = 0.9999577839307647 \n",
            "0.7310502783660559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS08FGqnFo2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9ff0ab8c-e49c-4a9b-dec8-9ef62bd20896"
      },
      "source": [
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.momentum_optimizer(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.04246583842484837\n",
            "m : 1.1022797253757213, c = 0.9671670283081942 \n",
            "0.7245544664645817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbP02fePqjTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f2caf7a6-713c-4f8d-f0c1-b00a433bae81"
      },
      "source": [
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.nesterov_acc_gradient(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.042460812781864254\n",
            "m : 1.1023342292131968, c = 0.9671005496655883 \n",
            "0.7245411987720427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IzBymBSHDam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "690a02f0-8b67-441f-9c93-dfc23cea3efa"
      },
      "source": [
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.adagrad(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.049702788395840906\n",
            "m : 1.018360553737217, c = 0.9817264134428917 \n",
            "0.727450640160768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejeBvNCcvBy2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "603c4d01-19f0-4a88-ab08-00d5816ca193"
      },
      "source": [
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.rms_prop(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.04131653003480039\n",
            "m : 1.1061391266385423, c = 0.8954461950550585 \n",
            "0.7100127961062376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4eNfESB4akQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cf60f576-4b1d-4896-be66-9ddc7e10a7f6"
      },
      "source": [
        "Y=[1,0,1,0,1,0,1,0,1,0]\n",
        "X=[1,-2,3,-4,5,-6,7,-8,9,-10]\n",
        "node=LogisticRegression()\n",
        "node.infer(2)\n",
        "node.adam(X, Y, epochs=100)\n",
        "print(node.infer(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log loss : 0.03953991935535437\n",
            "m : 1.1278470923447843, c = 0.8725116413899995 \n",
            "0.7052680494344187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9vCf6a1A6hE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}